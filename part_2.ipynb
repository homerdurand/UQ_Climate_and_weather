{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on : Perturbed initial conditions\n",
    "\n",
    "We have been observing that the chaotic nature of the system made impossible to accurately predict future extreme events as we have incomplete information about initial state. \n",
    "We now propose to implement a Large Ensemble (LE) of simulations via the Monte Carlo algorithm to better characterise our uncertainty in our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from l96 import *\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score \n",
    "from utils import  *\n",
    "from sklearn.decomposition import PCA\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colorblind-friendly colors\n",
    "color_sim = '#56B4E9'   # Light blue\n",
    "color_threshold = '#D55E00'  # Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "initX, initY = np.load('./data/initX.npy'), np.load('./data/initY.npy')\n",
    "l96_ref = L96TwoLevel(X_init=initX, Y_init=initY, save_dt=0.001, noYhist=False)\n",
    "l96_ref.iterate(30)\n",
    "l96_ref.erase_history()\n",
    "l96_ref.iterate(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l96_ref.erase_history()\n",
    "l96_ref.iterate(15)\n",
    "h = l96_ref.history\n",
    "X = h.X.values\n",
    "idx = np.arange(0, X.shape[0], 10)\n",
    "X_sampled = X[idx, :]\n",
    "# observations = X_sampled + np.random.normal(0, 0.3,  X_sampled.shape)\n",
    "\n",
    "Sigma = generate_positive_definite_matrix(k=36)\n",
    "E = np.random.multivariate_normal(np.repeat(0, l96_ref.K), 0.005*Sigma)\n",
    "observations = X_sampled + E\n",
    "\n",
    "threshold = np.percentile(observations, 99.9)\n",
    "\n",
    "X_init = observations[-1, :]\n",
    "Sigma = np.cov(observations.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the covariance of the observation\n",
    "# Is there correlated variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to properly choose the prior distribution?\n",
    "\n",
    "Here we assume that we are given the background noise covariance $\\Sigma$ and that the mean is $X_init$ (the last observation). \n",
    "* What would be the Maximum Entropy related to this constraints?\n",
    "* You can check [Maximum Entropy Distributions](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution)\n",
    "\n",
    "* Generate a large ensemble of simulation by sampling $n_{\\text{members}}$ over this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_members = ???\n",
    "B = ???\n",
    "\n",
    "X_perturbed = X_init + B\n",
    "\n",
    "# Generate samples with mean X_init and random noise following the Maximum entropy prior\n",
    "# Scale the covariance by a factor gamma as we wish to have a small uncertainty around X_init\n",
    "# You can try different values of gamma and see how this affect the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the past True values, the observations and the sampled points at location X[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we can propagate our uncertainty by runing the simulation over each possible initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input",
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "time = 5\n",
    "simulations = []\n",
    "for X_init in tqdm(X_perturbed):\n",
    "    l96_simulation = L96OneLevel(X_init=X_init, dt=0.001, noprog=True)\n",
    "\n",
    "    l96_simulation.iterate(time=time)\n",
    "    simulations.append(l96_simulation.history.X.data)\n",
    "\n",
    "    l96_simulation.erase_history()\n",
    "\n",
    "simulations = np.array(simulations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l96_ref.erase_history()\n",
    "l96_ref.iterate(time = time)\n",
    "X_true = l96_ref.history.X.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the the $5-95\\%$ range of our predictions together with ensemble mean and the observed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "plot_time_series(X_true[-1:, :], X_true, simulations, subsample_rate=30, alpha=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Did you use enough initial sample for your Monte-Carlo simulation to converge?\n",
    "* If we missed the extreme event, is it only because of the lack of initial samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "* By the Central Limit Theorem, $\\sqrt{n}(\\bar{Y}_n - \\mu_Y) \\to \\mathcal{N}(0, \\sigma^2)$\n",
    "* We can check convergence via convergence of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time span on which we want to measure the convergence\n",
    "time_0, time_1 = 3000, 4000\n",
    "\n",
    "# Plot the standard deviation evolution over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better sampling strategies \n",
    "* Using [scipy.stats.qmc](https://docs.scipy.org/doc/scipy/reference/stats.qmc.html), imtry other sampling strategies (e.g. Latin Hypercubes, Sobol sequences)\n",
    "* Do we have faster convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Post Processing\n",
    "* In practice, it is often impracticable to run thousands of simulations\n",
    "* We can get better estimation of the extremes using some post processing strategies F\n",
    "    * Fitting a GEV on a few ensmeble members (see [genextreme](https://docs.scipy.org/doc/scipy/tutorial/stats/continuous_genextreme.html))\n",
    "    * Using non-parametric density estimation with kernels density estimation (see [KDE](https://docs.scipy.org/doc/scipy/tutorial/stats/kernel_density_estimation.html))\n",
    "* Try to implement this post-processing strategy to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble mean performance\n",
    "* Check the ensemble mean performance compared to the first member by comparing MSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = X_true\n",
    "predictions = simulations.mean(axis=0)[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot both the ensemble mean and first member MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is the MSE adapted if we want to compare the all predictive distribution and the observations?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
